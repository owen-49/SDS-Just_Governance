# 一、pip-tools使用原因和介绍

## 1.. 背景与目标

- 目标是：**可维护（知道自己真正依赖什么）** + **可复现（任何机器装出来都一模一样）**。
- 这两个目标分别由两个文件承担：
  - **`.in`**：只写**顶层依赖**（你“想要”的），结构清晰、便于维护。
  - **`.txt`**：由工具**解析并锁定**顶层依赖的**全部传递依赖**（你“实际得到的”），实现可复现安装。

------

## 2. 为什么不直接维护一个 `requirements.txt`？

传统做法有两种，问题都很明显：

### 做法A：手写 `requirements.txt`（只写顶层）

- 问题：**不可复现**。`pip` 每次解析传递依赖可能拿到**不同版本**（上游发了个小版本就变了）。
- 结果：今天能跑，明天“同样的 requirements.txt”在 CI/同事机上突然挂了。

### 做法B：`pip freeze > requirements.txt`

- **freeze会读取你的虚拟环境并把你安装的所有依赖+版本全都写进requirements.txt。**

- 优点：复现度高（写满所有包的具体版本，**包括传递依赖**）。
- 致命点：
  - **难维护**：里面 100+ 行大多是**传递依赖**（不是你直装的），半年后你看不懂“我到底依赖了什么”。
  - **升级困难**：你只想升 `fastapi`，却要手动改一串相关包版本。
  - **混淆环境**：开发工具/临时试装的包也被 freeze 进去，越滚越脏。
  - **安全/一致性一般**：默认不带哈希校验；且你改一次环境就得重新 freeze。

> 结论：**单个 requirements.txt 无法同时满足“可维护 + 可复现”。**

------

## 3. 为什么用 `.in → .txt`（pip-tools 的核心思路）

**核心思想**：把“**意图**”与“**结果**”拆开。

- **`.in`（意图）**
  只列**顶层依赖** + 可选的**版本范围**。
  例子（与你项目吻合）：

  ```python
  # requirements/base.in
  fastapi
  uvicorn[standard]
  sqlalchemy>=2
  asyncpg                 # 异步驱动（应用）
  psycopg2-binary         # 同步驱动（Alembic）
  alembic
  httpx
  python-dotenv
  pydantic-settings
  passlib[bcrypt]
  email-validator
  pyjwt
  ```

- **`.txt`（结果）**
  用 `pip-compile` 从 `.in` **解析并锁定**：

  - 顶层依赖 + **全部传递依赖**
  - **精确版本**（可加哈希）
  - 可复现、可审计

  ```
  # requirements/base.txt（生成）
  fastapi==0.116.1
  starlette==0.38.5         # fastapi 的传递依赖
  pydantic==2.9.2
  sqlalchemy==2.0.35
  greenlet==3.1.0           # sqlalchemy 的传递依赖
  ...
  ```

- **安装与对齐：`pip-sync`**
  `pip-sync base.txt dev.txt` 会**精确安装**并**卸载多余**，让环境与 `.txt` **完全一致**（真正可复现）。

> 这样：**.in 让你“看得懂、改得动”；.txt 保证“装得准、跑得稳”。**

------

## 4. 顶层依赖 vs 传递依赖

- **顶层依赖**：你**主动**装的（或必须声明的）。如 `fastapi`, `sqlalchemy`, `asyncpg`, `psycopg2-binary`, `alembic`…
  其中 `asyncpg/psycopg2-binary` 很典型：你代码里不直接 `import`，但 **SQLAlchemy 会根据 URL 动态 import 驱动**；不装就报错（所以它们仍是你的**顶层依赖**）。
- **传递依赖**：因为顶层依赖**间接需要**才被装上的，如 `starlette/h11/greenlet/Mako` 等。**不要写在 `.in`**，让工具去锁就好。

> 你在 `site-packages` 里只会看到“所有”包，不区分层级，所以需要工具（pip-tools）来做**解析与锁定**。

------

## 5. 为什么不用 `freeze`：原理与代价

- `pip freeze` 把**当前环境里的所有包**（顶层 + 传递 + 临时试装）**一股脑写下**。
- 短期看是“可复现”，**长期看不清依赖结构，升级困难**，容易因历史残留包导致 CI/部署问题。
- 你要升一个顶层依赖版本，freeze 流程是**全量重写**，差异巨大，**代码审查难以把控**。

> `pip-tools` 的 `.in → .txt` 把“你关心的”和“机器关心的”分离，避免 freeze 的维护地狱。

------

## 6. 生成首个base.in / dev.in文件

项目已经写到一半了，或者我原来的requirements.txt不想要，怎么生成首个 `.in` 文件？（我已经生成，可以不看）

### 路线A（最佳方案，干净）——基于源码 import 反推，再手动补驱动/工具

1. 扫描源码得到“最小集合”：

   ```bash
   python -m pip install pipreqs	# pipreqs是扫描工具
   python -m pipreqs . --force --encoding=utf-8 --ignore .venv,venv
   	# 扫描当前目录源代码下所有import语句，从而推断出你主动import的顶层依赖
   # 会生成 requirements.txt
   ```

2. 然后，把生成的 `requirements.txt` 内容放入 `requirements/base.in`。

3. 但是，需要注意的是，这些扫描出来顶层依赖只是你在源代码中显式import的，还有一些依赖是你本来必须主动声明的包。比如数据库驱动，uvicorn，等等。

   这些依赖的其中一些，之所以你必须手动安装。很大程度上因为它们本质是间接依赖，但是又是可选的间接依赖。因为数据库驱动有很多种可选，服务器也有很多种可选，所以它们不会被传递性安装。你必须主动声明你要安装它。

   还有的是因为它们是单独的工具，而不是被你的源代码依赖的工具。比如alembic，当你在数据库迁移时，并没有用到你写的主程序，反而是alembic是主程序。主程序也用不到alembic，因此import肯定也扫不到。

4. **因此，还需手动补**：你虽不 `import` 但**必须声明**的包：

   - 数据库驱动：`asyncpg`, `psycopg2-binary`
   - 运行服务器：`uvicorn[standard]`
   - 迁移工具：`alembic`
   - 配置/校验/加密等你确实用到的。

> 弯路：直接拿 pipreqs 结果就用，**会漏**上述驱动/工具包（它只看 import）。

### 路线B（省事）——直接从你的虚拟环境的site-packages导出“顶层依赖”

```bash
python -m pip install pip-chill	 #pip-chill是扫描工具，扫描你的虚拟环境依赖目录
python -m pipchill > requirements\base.in
```

- **freeze扫的是所有依赖，而pip-chill只会扫顶层依赖（你主动import的）**
- 这一步会把你**手动装过**的包导出来（比 `freeze` 干净），但仍可能混入**工具链/历史包**（如 `jaraco.*`, `tinycss2`, `terminaltables`…）。
- 你要**删掉**明显不是运行时必需的项（或把它们移到 `dev.in`）。

> 弯路：**盲信 pip-chill**，把工具链/历史残留包全塞进 base.in，导致运行时膨胀。

### 路线C（全新项目）——直接拿“模板 `.in`”

对你这类栈，直接用开头给的模板（第2节的 `base.in`），按需加减，**亦可**。

------

## 6. 工具介绍

- **pipreqs**：读你的**源码**，解析 `import xxx`，映射到 PyPI 包名（有内置字典），生成“最小依赖”。
  - 优点：干净（只列代码确实用到的）。
  - 缺点：**不认识“运行时必须但不 import 的包”**（驱动/服务器/迁移工具等），会漏。
- **pip-chill**：从**当前环境**里找“你**手动安装过**的顶层包”，导出来。
  - 优点：能把你确实装过的列出。
  - 缺点：会带出**工具链/历史残留**（非运行时必需），需要人工筛。
- **pip-tools**（`pip-compile` / `pip-sync`）：
  - `pip-compile`：读取 `.in`，用 pip 的解析器把**传递依赖全解开**并**锁定精确版本**，生成 `.txt`。
  - `pip-sync`：对照 `.txt`，**安装缺的**、**卸载多的**，让环境与锁文件**完全一致**（真正“可复现”）。



# 二、建立虚拟环境，使用pip-tools安装依赖

下面这份是“把依赖管理一次性理顺”的**小白到进阶**教程。你按顺序做，就能在你的项目里稳定使用 `requirements/base.in`、`requirements/dev.in` + `pip-tools` 来安装和管理依赖（适配当前的 FastAPI + SQLAlchemy 异步 + Alembic 的栈）。

------

## 第一步： 重温两个核心概念

- **.in 文件（你写）**
  只列“**顶层依赖**”（你直接用到的包，和你希望的版本范围）。例：`fastapi`, `sqlalchemy>=2`。
- **.txt 文件（工具生成）**
  用 `pip-compile` 从 `.in` 解析出“**可复现**”的**完整清单**：顶层 + 全部子依赖 + **锁死版本**（可选带哈希）。团队/CI/生产**只**看 `.txt` 来安装。

> 心法：**.in 写意图，.txt 锁结果。**
> 改 `.in` → `pip-compile` 生成新的 `.txt` → `pip-sync` 安装/对齐环境。

------



## 第二步： 创建虚拟环境（如果创建了就跳过）

> 强烈建议：从**项目根**创建虚拟环境，用 **`python -m ...`** 形式确保命令跟着当前解释器走。

```bash
# 1) 确认 python 存在
python -V

# 2) 新建并激活虚拟环境（项目根）
python -m venv .venv
.\.venv\Scripts\Activate.ps1

# 3) 升级 pip + 安装依赖管理工具（跟着当前 python 走）
python -m pip install --upgrade pip
python -m pip install pip-tools
```

> 以后所有与 pip/pip-tools 相关命令，最好都用 `python -m ...`，避免“走错环境”。

------



## 第三步： 创建依赖目录与 `.in` 文件，从 `.in` 生成“锁定清单txt”

- **这一步我已经完成，只需找到backend/requirements/文件夹，然后做第四步即可。**

```
backend/
└─ requirements/
   ├─ base.in   # 生产运行时必需
   └─ dev.in    # 开发/测试工具（引用 base.in） #除了运行时需要的还有开发时需要的
```

> 原则：**运行时用的进 base.in**；**只在开发/测试阶段用的进 dev.in**。

```python
# 仍在激活的虚拟环境里
python -m piptools compile -o requirements\base.txt requirements\base.in
python -m piptools compile -o requirements\dev.txt  requirements\dev.in
```

- `base.txt` / `dev.txt` 会列出**全部包** + **精确版本**（和必要时的哈希）。
- 以后每次改 `.in`，都要重新 `compile` 一下生成新的 `.txt`。

------



## 第四步：把你自己的虚拟环境“对齐”锁定清单

```bash
python -m piptools sync requirements\base.txt requirements\dev.txt
```

- `pip-sync` 会**安装** `.txt` 里的包，**卸载**不在 `.txt` 的杂包，确保环境**整洁一致**。

------



## 第五步：日常操作（你以后基本只会用到这些）

​               **--------------------------------------------------------必读--------------------------------------------------------------**

1. **你开发用到一个依赖，要新增一个依赖，你不能只安装到你自己的虚拟环境，你得让大家都知道。因为git仓库不存虚拟环境。**

2. **在以前，你可能是先git install，然后才想起来写到requirements.txt里。**

3. **现在你必须先写进.in，然后重新编译出.txt文件，然后再重新对齐依赖。这样才能将版本确定，不然你的安装版本，和别人的安装版本可能是不一样的。**

```bash
# 1) 写进 .in（运行也用就写 base.in；只开发用就写 dev.in）
		# 运行和开发都用写到base.in就行，因为dev.in会引用base.in！
echo aiosmtplib >> requirements\base.in

# 2) 重新编译
python -m piptools compile -o requirements\base.txt requirements\base.in

# 3) 同步安装
python -m piptools sync requirements\base.txt requirements\dev.txt
```



------



# 三、怎么把弄乱的依赖重新编排

> **如果每个人都按上面第五步的要求来做，这一部分是用不到的。**

- **如果你没按照规范装了一堆包又忘记写到base.in或dev.in中，采取以下解决方案：**

**方法 A（更干净）**：基于源码 import 反推顶层依赖，再手动补工具

```bash
python -m pip install pipreqs
python -m pipreqs . --force --encoding=utf-8 --ignore .venv,venv
# 生成的 requirements.txt → 拷到 requirements/base.in
# 手动把运行时需要但源码不 import 的工具/驱动补进去（如 alembic、asyncpg、psycopg2-binary 等）
```

**方法 B（更省事）**：从当前环境导出“顶层依赖”

```bash
python -m pip install pip-chill
python -m pipchill > requirements\base.in
# 然后删掉明显是工具链/子依赖的项（如 tinycss2、jaraco.*、terminaltables 等）
# 再按本文模板补齐运行时必需项
```

然后**回到“四、五步”**：`pip-compile` → `pip-sync`，环境就收敛干净了。



# 四、Alembic简介

## 1. Alembic 是什么 & 使用的原因

Alembic 是 SQLAlchemy 官方的**数据库迁移工具**。它把“表结构的变化（DDL）”做成**一版一版**的脚本（可前进、可回退），让团队在 **dev / test / prod** 各环境保持**同一套结构**，并且**可审计、可回滚**。

**不用 Alembic 会怎样？**

- 你改完数据模型要生成数据库，在本地随便 `create_all()` 或手改库，队友那边对不上；
- 线上要改表靠手敲 SQL，容易漏、难回滚；
- 回溯“某次版本改了哪些列”几乎不可能。

**用了 Alembic，得到：**

- **版本化**：每次改表都会生成一个“迁移版本”；
- **一致性**：成员一条命令 `alembic upgrade head` 就同步结构；
- **可回滚**：出问题 `alembic downgrade -1` 立刻回退；
- **可审计**：每次变更有脚本、有说明，PR 可评审。

------

## 2. 原理

把 Alembic 想成“数据库结构的 Git”：

- **迁移脚本**：放在 `backend/migrations/versions/`，每个文件包含：

  ```py
  def upgrade():   # 升级到这一版：CREATE / ALTER / ADD INDEX ...
  def downgrade(): # 回退这一版：DROP / RENAME BACK ...
  ```

- **版本链**：每个脚本有 `revision`（类似 commit id）和 `down_revision`（前一个版本）。

- **当前版本**：数据库里会创建出一张表 `alembic_version`，记录“我现在在第几版”。

- **自动生成**（autogenerate）：Alembic对比**代码里的结构**（`Base.metadata`）和**数据库当前结构**，帮你生成“差异脚本”；你**必须手动 review** 后再执行。

- **执行迁移**：`alembic upgrade head` 让数据库向前跑到最新版本；`downgrade` 回退。

> 注意：Alembic不直接理解你的 ORM 类，它看的是**`Base.metadata` 里的表结构**（ORM 定义完成后产生的“表描述对象”）。

------

## 3. 目录结构

目录建议（我们已经按此风格搭建）：

```json
backend/
├─ app/
│  ├─ models/                 # SQLAlchemy 模型（继承统一 Base）
│  └─ core/
│     ├─ config.py            # 读取 .env（DATABASE_URL 等）
│     └─ db.py                # 运行期的 engine / Session（给 FastAPI 用）
├─ migrations/                # ← Alembic 目录（迁移执行期使用）
│  ├─ env.py                  # 指定 target_metadata、连接串来源
│  └─ versions/               # 每次 revision 都在这里
└─ .env                       # 本地环境变量配置（不提交）
```

**重要约定**

- `.env` 里提供**两套 URL**（同一个库、不同驱动）：

  ```json
  DATABASE_URL=postgresql+psycopg://user:pwd@localhost:5432/dbname      # 同步，用于 Alembic脚本访问数据库，更稳健
  ASYNC_DATABASE_URL=postgresql+asyncpg://user:pwd@localhost:5432/dbname # 异步，用于项目实际运行访问数据库，更加高效
  ```

- **迁移（Alembic）用同步驱动**更稳：在 `migrations/env.py` 用 `DATABASE_URL`。

- 运行期（FastAPI）用 `core/db.py` 管理 `engine`/`Session`；**迁移不使用**这个文件。

------

# 

# 五、Alembic实战

## 1. 第一次接入【已经完成】

> 在 `backend/` 目录里操作

1. **初始化（会生成 `migrations/` 目录）**

```
alembic init migrations
```

2. 配置 `migrations/env.py`

```python
# 关键片段（示意）
from alembic import context
from sqlalchemy import create_engine, pool

from app.models import Base                  # 我们的模型统一从这里导出 Base
from app.core.config import settings         # 我们的配置入口（读 .env）

config = context.config
target_metadata = Base.metadata              # 告诉 Alembic：以这套表结构为“真相”

def run_migrations_offline():
    url = settings.database_url              # 用同步 URL

    context.configure(
        url=url,
        target_metadata=target_metadata,
        literal_binds=True,
        compare_type=True,
        compare_server_default=True,
    )
    with context.begin_transaction():
        context.run_migrations()

def run_migrations_online():
    connectable = create_engine(
        settings.database_url,
        poolclass=pool.NullPool,
    )
    with connectable.connect() as connection:
        context.configure(
            connection=connection,
            target_metadata=target_metadata,
            compare_type=True,
            compare_server_default=True,
        )
        with context.begin_transaction():
            context.run_migrations()
```

3. 生成**首版**迁移（基线）

```bash
alembic revision --autogenerate -m "baseline schema"
```

打开 `migrations/versions/xxxx_baseline_schema.py`，**认真检查**：

- 表/列/索引/外键都对吗？
- `downgrade()` 能回退吗？
- 有些复杂的默认值/枚举/索引，必要时手工补充。

4. 执行迁移（把库建起来）

```bash
alembic upgrade head
```

数据库出现所有表 & `alembic_version`，OK！

> **从现在起，别再用 `Base.metadata.create_all()` 改结构。所有结构变更走 Alembic。**

------



## 2. 日常开发使用

**A. 改了模型（加列/改约束/新表）之后：**

```bash
alembic revision --autogenerate -m "users add phone"
# 打开新生成的脚本，检查无误
alembic upgrade head
```

**B. 回滚上一个版本：**

```bash
alembic downgrade -1
```

**C. 团队新同学拉仓库：**

```bash
cd backend
python -m piptools sync requirements\base.txt requirements\dev.txt
											# 带上 alembic/驱动等
alembic upgrade head                        # 一键把库升到最新结构即可
```

- **注意理解：你怎么更新回退，改的都是你连接的本地数据库的版本状态，不会对别人有任何影响，也不会对项目的Alembic配置和历史有任何影响。只有当你根据models生成revision脚本后，你的Alembic多了version，才会影响到Alembic版本控制。这时候你Git提交Alembic相关文件，大家就都能知道有了一个新版本。**

**D. CI/CD：**

- 部署前执行 `alembic upgrade head`；
- 失败可 `alembic downgrade <prev_revision>` 回退。

------



## 3. 命令速查

```bash
# 修改数据模型后，生成新迁移（自动对比 Base.metadata 与当前数据库）
alembic revision --autogenerate -m "add first_login_at to users"

# 升级到最新
alembic upgrade head

# 回退 1 步 / 回到起点
alembic downgrade -1
alembic downgrade base

# 查看当前版本 / 历史
alembic current
alembic history --verbose
```



## 4. 审核迁移（非常关键）

###  4.1 什么是 review？

**review = 你打开 Alembic 自动生成的迁移脚本，自己认真检查并修改它**

------

#### 举个真实流程：

你在 `models/user.py` 新加了一个字段：

```py
class User(Base):
    ...
    first_login_at: Mapped[datetime] = mapped_column(nullable=True)
```

然后你运行：

```py
alembic revision --autogenerate -m "add first_login_at to users"
```

这时会在：

```py
migrations/versions/20250921_abc123_add_first_login_at_to_users.py
```

生成一个迁移脚本，里面可能是：

```py
def upgrade():
    op.add_column('user', sa.Column('first_login_at', sa.DateTime(), nullable=True))

def downgrade():
    op.drop_column('user', 'first_login_at')
```

------

### 4.2 为什么要 review？

因为 Alembic 不是万能的，它可能：

| 情况                    | 你需要 review 的原因                                         |
| ----------------------- | ------------------------------------------------------------ |
| 生成语句有错            | 比如类型错、表名拼错、nullable 拼错                          |
| 复杂默认值              | Alembic 有时不会识别 `server_default=...`，需要你手动补      |
| 改了字段名              | Alembic 以为你删了 A 列、又加了一个新列 B，但其实你是改名，**自动生成会丢数据！** |
| 枚举                    | 有时 Alembic 无法正确生成 `CREATE TYPE` / `DROP TYPE`，需你手写 |
| 多对多中间表 / 联合主键 | 自动生成可能顺序不对，回滚失败                               |
| 索引/唯一约束           | 命名可能乱或重复，需要你对齐项目的 `naming_convention`       |

> ⚠️ 所以你必须“人工 review”：这就是 `revision --autogenerate` 之后的第二步。

------

### 4.3 可以修改 autogenerate 的内容吗？

当然可以，而且**你应该这么做**！

### 示例：

#### Autogenerate 给你这个：

```py
op.add_column('user', sa.Column('x', sa.Integer()))
```

#### 你可以手动改成：

```py
op.add_column('user', sa.Column('x', sa.Integer(), server_default='0'))
```

或者加个注释说明用途：

```py
# 添加用于记录第一次登录时间
op.add_column('user', sa.Column('first_login_at', sa.DateTime(), nullable=True))
```

> ✅ Alembic 不禁止你动迁移脚本，它的任务只是帮你“生成草稿”，不是“强行执行”。

**不要盲信自动生成**，重点看：

- **是否误删/误改了列（尤其是重命名时，Alembic 可能以为是“删除+新建”）。**
- **时间默认值 `server_default` 是否如你预期。**
- **`Enum`、`Check`、`FK` 的变化是否正确（某些场景需要手写 `op.execute(...)`）。**



## 5.实战注意事项（很有用）

1. **命名约定（强烈推荐）**
   在定义 `MetaData` 时设置 `naming_convention`，减少不同机器/不同版本下的“名称不一致”噪音：

   ```py
   from sqlalchemy import MetaData
   NAMING_CONVENTION = {
       "ix": "ix__%(column_0_label)s",
       "uq": "uq__%(table_name)s__%(column_0_name)s",
       "ck": "ck__%(table_name)s__%(constraint_name)s",
       "fk": "fk__%(table_name)s__%(column_0_name)s__%(referred_table_name)s",
       "pk": "pk__%(table_name)s",
   }
   metadata = MetaData(naming_convention=NAMING_CONVENTION)
   Base = declarative_base(metadata=metadata)
   ```

   这样 autogenerate 更稳定。

2. **重命名列/表**
   Alembic 无法自动识别“重命名”。会生成“删旧建新”。

   - 推荐手写：`op.alter_column(..., new_column_name="...")`（某些方言支持有限）。
   - 否则要迁移“数据丢失”的风险。

3. **修改 Enum**（Postgres）
   变更枚举值需手写：

   ```py
   op.execute("ALTER TYPE role ADD VALUE IF NOT EXISTS 'new_value'")
   ```

   删除枚举值更复杂，通常建新类型/替换/删旧类型。

4. **大表 DDL**
   尽量**可回滚**，并在低峰执行；Postgres 多数 DDL 事务内可回滚，但索引重建/类型转换要评估时长。

5. **数据迁移（非 DDL）**
   可以在迁移脚本里写：

   ```
   from sqlalchemy.sql import table, column
   users = table("users", column("id"), column("name"))
   op.execute(users.insert().values(id="...", name="admin"))
   ```

   或 `op.bulk_insert(...)`。注意回滚策略。

6. **多分支（merge）**
   多人并行生成迁移易出现多 head：

   ```
   alembic merge -m "merge heads" <revA> <revB>
   ```

   生成合并迁移，统一主线。